# Spotify 歌单导入工具测试策略

本文档概述了 "Spotify 歌单导入工具" 的测试策略，旨在确保软件的质量、功能的正确性以及用户体验的可靠性。

## 整体理念与目标

* **目标**:
    * 确保所有核心功能 (FR1-FR5，针对 Spotify) 按预期工作。
    * 预防代码变更引入的功能退化 (Regression)。
    * 提高代码的健壮性和对各种输入的容错能力。
    * 验证与 Spotify API 的正确集成和交互。
    * 建立对软件质量的信心，支持更安全的重构和功能迭代。
* **方法**:
    * 采用多层次的测试方法，包括单元测试、集成测试和手动/端到端测试。
    * 尽可能自动化测试过程，特别是在 CI/CD 流程中（如果设立）。
    * 重点测试业务逻辑、API 交互边界和用户输入处理。

## 测试级别

### 1. 单元测试 (Unit Tests)

* **范围**: 测试最小的可测试单元，如单个函数、方法或类，与其他部分隔离。重点关注：
    * `core/input_parser.py` 中的歌曲行解析逻辑 (各种有效和无效的输入格式)。
    * `core/models.py` 中数据模型的实例化和基本行为（如果存在）。
    * `spotify/client.py` 中辅助函数或内部逻辑的正确性（不直接调用实际 API）。
    * `utils/` 目录下的工具函数。
* **工具**:
    * `pytest` (推荐) 或 Python 内置的 `unittest` 模块。
* **Mocking/Stubbing**:
    * 使用 `pytest-mock` (配合 `pytest`) 或 `unittest.mock` 来模拟外部依赖 (例如，`spotipy` 客户端的某些方法，文件系统操作)。
    * 例如，在测试 `input_parser` 时，可以模拟 `open()` 内置函数来提供测试文件内容。
* **位置**: `tests/core/`, `tests/spotify/` 等目录下，文件名通常以 `test_` 开头 (例如 `test_input_parser.py`)。
* **期望**:
    * 覆盖所有重要的逻辑分支和边界条件。
    * 测试执行速度快，可以频繁运行（例如，每次代码提交前）。
    * 为每个模块提供较高的代码覆盖率。

### 2. 集成测试 (Integration Tests)

* **范围**: 验证多个组件之间的交互是否按预期工作。例如：
    * 测试 `main.py` 中从输入解析到调用 `SpotifyClient` 的流程（此时可以 mock 掉实际的 `spotipy` API 调用，但验证参数传递和方法调用顺序）。
    * 测试 `spotify/auth.py` 与 `spotipy.SpotifyOAuth` 的集成（可能较难完全自动化，或需要特定设置）。
    * 测试 `spotify/client.py` 中各方法（如搜索、创建播放列表、添加歌曲）在模拟 `spotipy` 客户端行为（返回预期的成功/错误响应）时的表现。
* **工具**:
    * `pytest`。
    * `pytest-mock` 或 `unittest.mock` 用于模拟 `spotipy` 对象的方法，使其返回预设的成功或失败数据，从而测试 `SpotifyClient` 如何处理这些响应。
* **位置**: `tests/spotify/` 或 `tests/integration/`。
* **期望**:
    * 关注模块间的接口、数据流和协作。
    * 确保组件间的契约得到遵守。
    * 执行速度通常慢于单元测试，但快于端到端测试。

### 3. 端到端测试 (End-to-End / E2E Tests) - 手动为主

* **范围**: 从用户的角度测试整个应用程序的完整流程。这通常涉及运行实际的 CLI 命令，并与真实的 Spotify API (使用测试账户或可清理的账户数据) 进行交互。
* **方法**:
    * **手动测试**:
        1.  准备包含各种情况的歌曲列表测试文件（正常歌曲、包含特殊字符的歌曲、格式错误的行、Spotify 上不存在的歌曲等）。
        2.  在本地环境中配置好 `.env` 文件（使用一个专门用于测试的 Spotify 账户凭据，或者一个您不介意在其中创建测试播放列表的账户）。
        3.  执行 `python main.py <测试文件>` 命令，使用不同的命令行选项。
        4.  检查控制台输出是否符合预期。
        5.  检查生成的匹配报告文件内容是否正确。
        6.  登录到 Spotify 账户，验证播放列表是否按预期创建、歌曲是否正确添加、播放列表的公开/私有状态和描述是否正确。
        7.  测试认证流程（首次授权、令牌刷新）。
        8.  测试各种错误情况（如无效的API密钥、网络中断（如果可能模拟）、API速率限制（较难主动触发和测试））。
    * **自动化 E2E (可选，较复杂)**: 对于 CLI 应用，可以使用 `subprocess` 模块调用脚本，并检查标准输出/错误和生成的文件。但与外部服务（如 Spotify）的真实交互会使测试变慢、不稳定，并可能产生副作用（如在 Spotify 账户中留下测试数据）。通常对于此类项目，手动 E2E 测试结合良好的单元和集成测试是更务实的做法。
* **位置**: 测试步骤和场景应记录在测试计划文档或 `README.md` 的测试部分。
* **期望**:
    * 验证整个用户流程是否顺畅，所有组件是否能正确协同工作。
    * 发现单元测试和集成测试可能遗漏的问题。
    * 由于依赖外部服务和手动操作，这类测试执行频率较低（例如，在版本发布前）。

## 测试数据管理

* **单元/集成测试**:
    * 使用固定的、小型的测试数据。例如，在 `tests/test_data/` 目录下存放小的歌曲列表文件，或在测试代码中直接定义输入字符串列表。
    * Mock 对象应返回预定义的、可控的响应数据。
* **手动 E2E 测试**:
    * 准备一些具有代表性的歌曲列表文件，包含有效、无效、生僻、常见的歌曲。
    * 建议使用一个专门的 Spotify 测试账户，以避免干扰个人主账户，并方便清理测试生成的播放列表。

## CI/CD 集成 (如果设立)

* **自动化测试执行**:
    * 在持续集成 (CI) 流程中（例如使用 GitHub Actions），自动运行所有单元测试和可自动化的集成测试。
    * 每当有代码推送到主分支或 Pull Request 时触发。
* **代码覆盖率报告**:
    * 集成代码覆盖率工具 (如 `pytest-cov` 生成的报告，并上传到 Codecov.io 等服务) 来监控测试覆盖情况。
* **构建与打包 (可选)**:
    * CI 流程也可以包含构建可执行文件（如果采用 `PyInstaller` 等）的步骤。
* **流水线失败**: 如果任何自动化测试失败，CI 流水线应标记为失败，阻止问题代码的合并。

## 变更日志

| 变更描述             | 日期       | 版本 | 作者        |
| -------------------- | ---------- | ---- | ----------- |
| 初稿 - Spotify-only | 2025-05-17 | 0.1  | 3-Architect |
| ...                  | ...        | ...  | ...         |